{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "2_facial_keypoints.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jusnelda/i2dl_colab/blob/master/2_facial_keypoints.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQSYWlUhuKJh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "795a9fd0-6f7e-40ff-fcb0-027000c984f6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My\\ Drive/i2dl/\n",
        "#!git clone https://github.com/jusnelda/i2dl_colab.git\n",
        "%cd /content/drive/My Drive/i2dl/i2dl_colab/exercise_3\n",
        "!git pull"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/i2dl\n",
            "fatal: destination path 'i2dl_colab' already exists and is not an empty directory.\n",
            "/content/drive/My Drive/i2dl/i2dl_colab/exercise_3\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 4 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (4/4), done.\n",
            "From https://github.com/jusnelda/i2dl_colab\n",
            "   ea1ae9a..b8234cf  master     -> origin/master\n",
            "Updating ea1ae9a..b8234cf\n",
            "Fast-forward\n",
            " exercise_3/1_segmentation_nn.ipynb | 1137 \u001b[32m+++++++++++++++++++++++++++\u001b[m\u001b[31m---------\u001b[m\n",
            " 1 file changed, 847 insertions(+), 290 deletions(-)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjBOUKwus2cH",
        "colab_type": "text"
      },
      "source": [
        "# Facial Keypoint Detection\n",
        "  \n",
        "This project will be all about defining and training a convolutional neural network to perform facial keypoint detection, and using computer vision techniques to transform images of faces.  The first step in any challenge like this will be to load and visualize the data you'll be working with. \n",
        "\n",
        "Let's take a look at some examples of images and corresponding facial keypoints.\n",
        "\n",
        "<img src='key_pts_example.png' width=50% height=50%/>\n",
        "\n",
        "Facial keypoints (also called facial landmarks) are the small magenta dots shown on each of the faces in the image above. In images, there are faces and **keypoints, with coordinates (x, y)**.  These keypoints mark important areas of the face: the eyes, corners of the mouth, the nose, etc. These keypoints are relevant for a variety of tasks, such as face filters, emotion recognition, pose recognition, and so on.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkB_dBZCs2cY",
        "colab_type": "text"
      },
      "source": [
        "## Load and Visualize Data\n",
        "\n",
        "The first step in working with any dataset is to become familiar with your data; you'll need to load in the images of faces and their keypoints and visualize them! \n",
        "\n",
        "#### Training and Testing Data\n",
        "\n",
        "This facial keypoints dataset consists of thousands gray images. All of these images are separated into either a training or a test set of data.\n",
        "\n",
        "* training images are for you to use as you create a model to predict keypoints.\n",
        "* test images are unknown to you, which will be used to test the accuracy of your model.\n",
        "\n",
        "The information about the images and keypoints in this dataset are summarized in CSV files, which we can read in using `pandas`. Let's read the training CSV and get the annotations in an (N, 2) array where N is the number of keypoints and 2 is the dimension of the keypoint coordinates (x, y).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_Zz-SNcs2cj",
        "colab_type": "text"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        "    <h3>Note</h3>\n",
        "    <p>Sometimes you will be presented with csv data. The default python csv reader is ok, but for more efficient data manipulation, it is recommended to use <b>pandas</b>.</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "lz2IM6Uws2cq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the required libraries\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from exercise_code.vis_utils import show_all_keypoints\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# for auto-reloading external modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoBCu91rs2c6",
        "colab_type": "text"
      },
      "source": [
        "### Training data size\n",
        "\n",
        "The training data is in csv file. There are 6750 images in training data, where the first 30 columns are the keypoints coordinates (x, y),\n",
        "and the 31th column is the image content. We provide you the function `get_image` and `get_keypoints` function to get i-th image and i-th set of keypoints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHAqoHcIs2c_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "key_pts_frame = pd.read_csv('../../datasets/landmark_data/training.csv')\n",
        "key_pts_frame.describe().loc['count'].plot.bar()\n",
        "print(key_pts_frame.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkK9vrErs2dH",
        "colab_type": "text"
      },
      "source": [
        "From visulization, we can see not every image contains full landmarks, therefore we need to delete those without full landmarks. And after dropping, we have almost 2k images left as our training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzAlla37s2dJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "key_pts_frame.dropna(inplace=True)\n",
        "key_pts_frame.describe().loc['count'].plot.bar()\n",
        "print(key_pts_frame.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvkYpKGks2dR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "058b99f6-24ca-4529-e6dc-8515aea08968"
      },
      "source": [
        "from exercise_code.data_utils import get_keypoints\n",
        "from exercise_code.data_utils import get_image\n",
        "\n",
        "n = 0\n",
        "image = get_image(n, key_pts_frame)\n",
        "keypoints = get_keypoints(n, key_pts_frame)\n",
        "print('image size: {}'.format(image.shape))\n",
        "print('keypoints size: {}'.format(keypoints.shape))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "image size: (96, 96)\n",
            "keypoints size: (15, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_-DVKjIs2da",
        "colab_type": "text"
      },
      "source": [
        "Below, is a function `show_all_keypoints` that takes in an image and keypoints and displays them.  As you look at this data, **note that these images are not all of the same quality**!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "V9RfSpPvs2dc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# select an image by index in our data frame\n",
        "for n in range(5):\n",
        "    image = get_image(n, key_pts_frame)\n",
        "    key_pts = get_keypoints(n, key_pts_frame)\n",
        "    show_all_keypoints(image, key_pts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9phsfUts2di",
        "colab_type": "text"
      },
      "source": [
        "## Dataset class and Transformations\n",
        "\n",
        "To prepare our data for training, we'll be using PyTorch's Dataset class. Much of this this code is a modified version of what can be found in the [PyTorch data loading tutorial](http://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
        "\n",
        "#### Dataset class\n",
        "\n",
        "``torch.utils.data.Dataset`` is an abstract class representing a\n",
        "dataset. This class will allow us to load batches of image/keypoint data, and uniformly apply transformations to our data, such as rescaling and normalizing images for training a neural network.\n",
        "\n",
        "\n",
        "Your custom dataset should inherit ``Dataset`` and override the following\n",
        "methods:\n",
        "\n",
        "-  ``__len__`` so that ``len(dataset)`` returns the size of the dataset.\n",
        "-  ``__getitem__`` to support the indexing such that ``dataset[i]`` can\n",
        "   be used to get the i-th sample of image/keypoint data.\n",
        "\n",
        "Let's create a dataset class for our face keypoints dataset. We will\n",
        "read the CSV file in ``__init__`` but leave the reading of images to\n",
        "``__getitem__``.\n",
        "\n",
        "A sample of our dataset will be a dictionary\n",
        "``{'image': image, 'keypoints': key_pts}``. Our dataset will take an\n",
        "optional argument ``transform`` so that any required processing can be\n",
        "applied on the sample. We will see the usefulness of ``transform`` in the future. \n",
        "#### TODO: Implement a custom dataset for face keypoints in `exercise_code/dataloader.py` file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmrmnFaR1BPV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "bc26847b-f7e1-449e-b459-91f8beafe191"
      },
      "source": [
        "!git pull"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "*** Please tell me who you are.\n",
            "\n",
            "Run\n",
            "\n",
            "  git config --global user.email \"you@example.com\"\n",
            "  git config --global user.name \"Your Name\"\n",
            "\n",
            "to set your account's default identity.\n",
            "Omit --global to set the identity only in this repository.\n",
            "\n",
            "fatal: unable to auto-detect email address (got 'root@11784a1f302d.(none)')\n",
            "Cannot save the current index state\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "RTcrz59Js2dk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "62585012-f5cd-4f8a-aa74-1b965a60bfa3"
      },
      "source": [
        "from exercise_code.dataloader import FacialKeypointsDataset\n",
        "\n",
        "# Construct the dataset\n",
        "face_dataset = FacialKeypointsDataset(csv_file='../../datasets/landmark_data/training.csv')\n",
        "\n",
        "# print some stats about the dataset\n",
        "print('Length of dataset: ', len(face_dataset))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of dataset:  1546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXKzOoDPs2ds",
        "colab_type": "text"
      },
      "source": [
        "Now that we've defined this class, let's instantiate the dataset and display some images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "hFI4_8aDs2dt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "2b1e96b6-3817-4d8c-8237-7b56c85ce408"
      },
      "source": [
        "# Display a few of the images from the dataset\n",
        "num_to_display = 3\n",
        "\n",
        "for i in range(num_to_display):\n",
        "    \n",
        "    # define the size of images\n",
        "    fig = plt.figure(figsize=(20,10))\n",
        "    \n",
        "    # randomly select a sample\n",
        "    rand_i = np.random.randint(0, len(face_dataset))\n",
        "    sample = face_dataset[rand_i]\n",
        "\n",
        "    # print the shape of the image and keypoints\n",
        "    print('index: {}'.format(i))\n",
        "    print('image size: {}'.format(sample['image'].shape))\n",
        "    print('keypoint shape: {}'.format(sample['keypoints'].shape))\n",
        "\n",
        "    ax = plt.subplot(1, num_to_display, i + 1)\n",
        "    ax.set_title('Sample #{}'.format(i))\n",
        "    \n",
        "    # Using the same display function, defined earlier\n",
        "    show_all_keypoints(sample['image'][0], sample['keypoints'])\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "index: 0\n",
            "image size: (96, 96)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-12032be3c3b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'image size: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'keypoint shape: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keypoints'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_to_display\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'keypoints'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x720 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_bg7Y6ws2dz",
        "colab_type": "text"
      },
      "source": [
        "## Transforms\n",
        "\n",
        "Neural networks often expect images that are standardized; a fixed size, with a normalized range for color ranges and coordinates, and (for PyTorch) converted from numpy lists and arrays to Tensors.\n",
        "\n",
        "Therefore, we will need to write some pre-processing code.\n",
        "Let's create 2 transforms:\n",
        "\n",
        "-  ``Normalize``: to convert grayscale values with a range of [0, 1] and normalize the keypoints to be in a range of about [-1, 1]\n",
        "-  ``ToTensor``: to convert numpy images to torch images.\n",
        "\n",
        "\n",
        "We will use the default transforms from torchvision\n",
        "\n",
        "Observe below how these transforms are generally applied to both the image and its keypoints.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MLrG1I-s2d1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchvision import transforms, utils\n",
        "# tranforms\n",
        "\n",
        "from exercise_code.transforms import Normalize, ToTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMk30Hgps2d9",
        "colab_type": "text"
      },
      "source": [
        "## Test out the transforms\n",
        "\n",
        "Let's test these transforms out to make sure they behave as expected. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ifbWYhXs2d_",
        "colab_type": "text"
      },
      "source": [
        "## Create the transformed dataset\n",
        "\n",
        "Apply the transforms in order to normalized images and landmarks. Verify that your transform works by printing out the shape of the resulting data (printing out a few examples should show you a consistent tensor size).\n",
        "\n",
        "#### Hint: you can also implement or use some other default PyTorch data augmenatiton methods to improve your scores! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc93eNbbs2eB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the data tranform\n",
        "# order matters! i.e. rescaling should come before a smaller crop\n",
        "data_transform = transforms.Compose([Normalize(),\n",
        "                                     ToTensor()])\n",
        "\n",
        "# create the transformed dataset\n",
        "transformed_dataset = FacialKeypointsDataset(csv_file='../datasets/landmark_data/training.csv',\n",
        "                                             transform=data_transform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r_HzA7Ms2eH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print some stats about the transformed data\n",
        "print('Number of images: ', len(transformed_dataset))\n",
        "\n",
        "# make sure the sample tensors are the expected size\n",
        "for i in range(1):\n",
        "    sample = transformed_dataset[i]\n",
        "    print(i, sample['image'].size(), sample['keypoints'].size())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f1wKGWRs2eN",
        "colab_type": "text"
      },
      "source": [
        "## Data Iteration and Batching\n",
        "\n",
        "Right now, we are iterating over this data using a ``for`` loop, but we are missing out on a lot of PyTorch's dataset capabilities, specifically the abilities to:\n",
        "\n",
        "-  Batch the data\n",
        "-  Shuffle the data\n",
        "-  Load the data in parallel using ``multiprocessing`` workers.\n",
        "\n",
        "``torch.utils.data.DataLoader`` is an iterator which provides all these\n",
        "features, and we'll see how this is used, when we load data in batches to train a neural network! Then we can define a DataLoader class from PyTorch with transformed dataset, with batch size 4, shuffle functionality and multithreading with 4 workers.\n",
        "\n",
        "#### Note: num_workers > 0 might throw an error on machines without cuda. You might consider leaving num_workers=0 if you are working on CPU\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xvmFBzQs2eO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "batch_size = 20\n",
        "train_loader = DataLoader(transformed_dataset, \n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True, \n",
        "                          num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K69LDD-ws2eU",
        "colab_type": "text"
      },
      "source": [
        "## CNN Architecture\n",
        "\n",
        "Recall that CNN's are defined by a few types of layers:\n",
        "* Convolutional layers\n",
        "* Maxpooling layers\n",
        "* Fully-connected layers\n",
        "\n",
        "You are required to use the above layers and encouraged to add multiple convolutional layers and things like dropout layers that may prevent overfitting. You are also encouraged to look at literature on keypoint detection, such as [this paper](https://arxiv.org/pdf/1710.00977.pdf), to help you determine the structure of your network.\n",
        "\n",
        "\n",
        "#### TODO: Define your model in the provided file `exercise_code/classifiers/keypoint_nn.py` file\n",
        "\n",
        "This file is mostly empty but contains the expected name and some TODO's for creating your model.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "xuGw1yq7s2eX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from exercise_code.classifiers.keypoint_nn import KeypointModel\n",
        "model = KeypointModel()\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "6pdPcmZks2eb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display a few output samples from our network\n",
        "def show_sample_outputs(image, keypoints):    \n",
        "    # define the size of images\n",
        "    fig = plt.figure(figsize=(20,20))\n",
        "     \n",
        "    # Predict with model\n",
        "    predicted_keypoints = model(image)[0]\n",
        "    # Cast back to (x,y)-coordinates\n",
        "    predicted_keypoints = predicted_keypoints.view(-1, 2).detach()\n",
        "\n",
        "    # Undo data normalization\n",
        "    image = torch.squeeze(image) * 255.\n",
        "    keypoints = (keypoints.view(-1, 2) * 48) + 48\n",
        "    predicted_keypoints = (predicted_keypoints * 48) + 48\n",
        "\n",
        "    # print the shape of the image and keypoints\n",
        "    print('index: {}'.format(i))\n",
        "    print('image shape: {}'.format(image.shape))\n",
        "    print('gt keypoints shape: {}'.format(keypoints.shape))\n",
        "    print('predict keypoints shape: {}'.format(predicted_keypoints.shape))\n",
        "    \n",
        "    # Print data loader image\n",
        "    ax = plt.subplot(4, 1, 1)\n",
        "    ax.set_title('Sample #{}: Dataloader'.format(i))\n",
        "    # Using the same display function, defined earlier\n",
        "    show_all_keypoints(image, keypoints)\n",
        "\n",
        "    # Print predicted image\n",
        "    ax = plt.subplot(1, 1, 1)\n",
        "    ax.set_title('Sample #{}: Prediction'.format(i))\n",
        "    # Using the same display function, defined earlier\n",
        "    show_all_keypoints(image, predicted_keypoints)\n",
        "    plt.show()\n",
        "    \n",
        "num_to_display = 3\n",
        "for idx, sample in enumerate(train_loader):\n",
        "    if idx == num_to_display:\n",
        "        break\n",
        "    show_sample_outputs(sample['image'][0:1], sample['keypoints'][0:1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMpwOypJs2ei",
        "colab_type": "text"
      },
      "source": [
        "## Training your network\n",
        "We define the sovler for you to use, basically it takes out the data from the DataLoader, and via PyTorch functions to run forward and backward pass. If you want define your own solver, it is also welcome, feel free to change to achieve your best scores)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "-_3RTbTrs2ek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "from exercise_code.classifiers.keypoint_nn import KeypointModel\n",
        "\n",
        "def train_net(n_epochs):\n",
        "\n",
        "    # prepare the net for training\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "        # train on batches of data, assumes you already have train_loader\n",
        "        for batch_i, data in enumerate(train_loader):\n",
        "            # get the input images and their corresponding labels\n",
        "            images = data['image']\n",
        "            key_pts = data['keypoints']\n",
        "\n",
        "            # flatten pts\n",
        "            key_pts = key_pts.view(key_pts.size(0), -1)\n",
        "\n",
        "            # convert variables to floats for regression loss\n",
        "            key_pts = key_pts.type(torch.FloatTensor)\n",
        "            images = images.type(torch.FloatTensor)\n",
        "            #print(key_pts[0], images[0])\n",
        "\n",
        "            # forward pass to get outputs\n",
        "            output_pts = model(images)\n",
        "\n",
        "            # calculate the loss between predicted and target keypoints\n",
        "            loss = criterion(output_pts, key_pts)\n",
        "\n",
        "            # zero the parameter (weight) gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # backward pass to calculate the weight gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # update the weights\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # print loss statistics\n",
        "            # to convert loss into a scalar and add it to the running_loss, use .item()\n",
        "            if batch_i % 10 == 9:    # print every 10 batches\n",
        "                print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1,\n",
        "                                                                   batch_i + 1,\n",
        "                                                                   running_loss / (len(train_loader) * epoch + batch_i)))\n",
        "            \n",
        "            #return images, key_pts, output_pts\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "    \n",
        "# Load model and run the solver\n",
        "model = KeypointModel()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-6, nesterov=True)\n",
        "train_net(n_epochs=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHXWckEds2et",
        "colab_type": "text"
      },
      "source": [
        "## Display the trained outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "dlAcqwges2ew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#show_sample_outputs(images[0:1], key_pts[0])\n",
        "\n",
        "num_to_display = 3 \n",
        "for i in range(num_to_display):\n",
        "    sample = next(iter(train_loader))\n",
        "    show_sample_outputs(sample['image'][0:1], sample['keypoints'][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81KwXD5Bs2e0",
        "colab_type": "text"
      },
      "source": [
        "## Validation on model\n",
        "For our experiments, we use mean squared error (MSE) between the ground truth keypoint coordinate vector and the predicted keypoint coordinate vector. Given the ground truth vector and predict vector, MSE loss is defined as the average of the square of all of the error. \n",
        "\n",
        "#### Notice: the metric in this validation section is the same metric used in test evaluation.  You have to achieve more than 100 to get bonus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "l6KNJ8sUs2e1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def VAL_net():\n",
        "    # prepare the net for training\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # train on batches of data, assumes you already have train_loader\n",
        "    for batch_i, data in enumerate(val_loader):\n",
        "        # get the input images and their corresponding labels\n",
        "        images = data['image']\n",
        "        key_pts = data['keypoints']\n",
        "\n",
        "        # flatten pts\n",
        "        key_pts = key_pts.view(key_pts.size(0), -1)\n",
        "\n",
        "        # convert variables to floats for regression loss\n",
        "        key_pts = key_pts.type(torch.FloatTensor)\n",
        "        images = images.type(torch.FloatTensor)\n",
        "\n",
        "        # forward pass to get outputs\n",
        "        output_pts = model(images)\n",
        "\n",
        "        # calculate the loss between predicted and target keypoints\n",
        "        loss = criterion(output_pts, key_pts)\n",
        "\n",
        "        # print loss statistics\n",
        "        # to convert loss into a scalar and add it to the running_loss, use .item()\n",
        "        running_loss += loss.item()\n",
        "    # metric is 1 / (2 * MSE_loss)\n",
        "    metric = 1.0 / (2 * (running_loss/len(val_loader)))\n",
        "    print(\"Metric on VAL data: {}\".format(metric))\n",
        "    print('Finished Validation')\n",
        "    \n",
        "# create the validation dataset\n",
        "\n",
        "VAL_dataset = FacialKeypointsDataset(csv_file='../datasets/landmark_data/val.csv',\n",
        "                                     transform=data_transform)\n",
        "val_loader = DataLoader(VAL_dataset, \n",
        "                        batch_size=20,\n",
        "                        shuffle=True, \n",
        "                        num_workers=4,\n",
        "                        drop_last=True,\n",
        "                       )\n",
        "criterion = nn.MSELoss()\n",
        "VAL_net()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhzcAClPs2e8",
        "colab_type": "text"
      },
      "source": [
        "## Save the Model\n",
        "\n",
        "When you are satisfied with your training, save the model for submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5h1mWPM1s2e9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"models/keypoints_nn.model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yAGw0P2s2fB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}